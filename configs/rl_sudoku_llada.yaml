experiment:
    project_name: "rl_sudoku_llada1"
    project_dir: "/data/czx/rl_sudoku_llada1"
    logging_dir: "/data/czx/rl_sudoku_llada1/logs"
    deepspeed_file: "1_node_4_gpus_deepspeed_zero2"
    save_training_state: true
    resume_from_checkpoint: "/data/czx/rl_sudoku_llada/ckpt/optimized/step_000080"
    num_machines: 1
    machine_rank: 0
    main_process_ip: "127.0.0.1"
    main_process_port: 29500

model:
    pretrained_model: "/ssd/hf_home/hub/models--GSAI-ML--LLaDA-8B-Instruct/snapshots/9275bf8f5a5687507189baf4657e91c51b2be338"
    optimized_name: "optimized"

dataset:
    path: "data/train_dataset.jsonl"
    val_path: "data/val_dataset.jsonl"

training:
    gradient_accumulation_steps: 1
    batch_size: 4
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 114
    num_train_epochs: 1
    max_grad_norm: 1.0
    group_size: 8
####
    rollout_batch_size: 4
    micro_batch_size: 8
####
    gamma: 0.95
    lambda_coeff: 0.5
    clip_epsilon: 0.2
    epsilon_small: 1e-9
    location_temperature: 0.4
    token_temperature: 0.6
    mask_token: "<|mdm_mask|>"
    pad_token: "<|endoftext|>"
    sft_loss_scale: 1.0
    max_generation_length: 128
    validation_interval: 6
    checkpoint_interval: 20
    updates_per_rollout: 2

optimizer:
    learning_rate: 1e-6
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0
    epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    warmup_steps: 10
    min_lr_scale: 0.3

wandb:
    resume: False
    entity: null
    run_id: null

