experiment:
    project: "rl_sudoku_llada"
    logging_dir: "rl_sudoku_llada/logs"

model:
    pretrained_model: "/abs/path/of/model"
    optimized_name: "optimized"

dataset:
    path: "data/sudoku_prompts.jsonl"

training:
    gradient_accumulation_steps: 2
    batch_size: 2
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 42
    num_train_epochs: 1
    max_grad_norm: 1.0
    group_size: 4
    gamma: 0.95
    lambda_coeff: 0.5
    clip_epsilon: 0.2
    epsilon_small: 1e-9
    location_temperature: 0.05
    token_temperature: 0.6
    mask_token: "<|mdm_mask|>"
    pad_token: "<|endoftext|>"
    sft_loss_scale: 1.0

optimizer:
    learning_rate: 1e-5
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0
    epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    warmup_steps: 0
    min_lr_scale: 1.0

wandb:
    resume: False
    entity: null
    run_id: null

