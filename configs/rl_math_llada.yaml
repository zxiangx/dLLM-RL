experiment:
    project_name: "rl_math_llada_1207_ablation"
    project_dir: "./projects/rl_math_llada_1207_ablation"
    logging_dir: "./projects/rl_math_llada_1207_ablation/logs"
    deepspeed_file: "1_node_8_gpus_deepspeed_zero1"
    save_training_state: true
    resume_from_checkpoint: null
    num_machines: 1
    machine_rank: 0
    main_process_ip: "127.0.0.1"
    main_process_port: 29500

model:
    pretrained_model: "/data/home/scyf781/czx/LLaDA-8B-Instruct"
    optimized_name: "optimized"

dataset:
    path: "math_data/train_math_new.jsonl"
    val_path: "math_data/val_math.jsonl"
    test_path: "math_data/test_math.jsonl"

training:
    gradient_accumulation_steps: 512
    batch_size: 1 # 采样的batch
    train_batch_size: 8 # 训练的batch
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 42
    num_train_epochs: 1
    max_grad_norm: 1.0
    group_size: 16
    rollout_batch_size: 8
    micro_batch_size: 4
    gamma: 1.0
    token_clip_epsilon_high: 0.25
    token_clip_epsilon_low: 0.15
    locate_clip_epsilon_high: 0.25
    locate_clip_epsilon_low: 0.15
    epsilon_small: 1e-9
    tokens_per_step: 2
    kl_coefficient: 0.04

    evaluation_tokens_per_step: 4
    evaluation_group_size: 8

    location_temperature: 0.1
    token_temperature: 0.6
    eval_location_temperature: 0.1
    eval_token_temperature: 0.2
    mask_token: "<|mdm_mask|>"
    pad_token: "<|endoftext|>"
    max_generation_length: 256
    validation_interval: 8
    checkpoint_interval: 60
    updates_per_rollout: 8 # 每次rollout拆成多少次更新，覆盖当前样本各一次
    train_passes_per_sample: 1 # 一个sample重复训练几轮
    reuse_before_sample: false # 之前的sample还可以再利用
    enable_prompt_cache: true
    filter_invalid: true
    debug: false
    debug_rollout: true
    shuffle: true
    debug_rollout_store_path: "./projects/rl_math_llada_1207_ablation/debug_rollouts"
    debug_ratio: false
    debug_ratio_path: "debug_ratio.json"
    ablation: true

optimizer:
    learning_rate: 3e-6
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0
    epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    warmup_steps: 8
    min_lr_scale: 0.5

wandb:
    resume: False
    entity: null
    run_id: null
